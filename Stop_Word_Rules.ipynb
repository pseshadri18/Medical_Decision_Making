{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Word Rules\n",
    "\n",
    "We need to decide on a comprehensive list of stop words. To ensure our study's reproducibility, we must create this list of stop words using a defined set of rules. If a word fits these rules, then it may be deemed a stop word and removed from our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by getting the standard list of stop words from the nltk package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/samanthagarland/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "you're\n",
      "you've\n",
      "you'll\n",
      "you'd\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "he\n",
      "him\n",
      "his\n",
      "himself\n",
      "she\n",
      "she's\n",
      "her\n",
      "hers\n",
      "herself\n",
      "it\n",
      "it's\n",
      "its\n",
      "itself\n",
      "they\n",
      "them\n",
      "their\n",
      "theirs\n",
      "themselves\n",
      "what\n",
      "which\n",
      "who\n",
      "whom\n",
      "this\n",
      "that\n",
      "that'll\n",
      "these\n",
      "those\n",
      "am\n",
      "is\n",
      "are\n",
      "was\n",
      "were\n",
      "be\n",
      "been\n",
      "being\n",
      "have\n",
      "has\n",
      "had\n",
      "having\n",
      "do\n",
      "does\n",
      "did\n",
      "doing\n",
      "a\n",
      "an\n",
      "the\n",
      "and\n",
      "but\n",
      "if\n",
      "or\n",
      "because\n",
      "as\n",
      "until\n",
      "while\n",
      "of\n",
      "at\n",
      "by\n",
      "for\n",
      "with\n",
      "about\n",
      "against\n",
      "between\n",
      "into\n",
      "through\n",
      "during\n",
      "before\n",
      "after\n",
      "above\n",
      "below\n",
      "to\n",
      "from\n",
      "up\n",
      "down\n",
      "in\n",
      "out\n",
      "on\n",
      "off\n",
      "over\n",
      "under\n",
      "again\n",
      "further\n",
      "then\n",
      "once\n",
      "here\n",
      "there\n",
      "when\n",
      "where\n",
      "why\n",
      "how\n",
      "all\n",
      "any\n",
      "both\n",
      "each\n",
      "few\n",
      "more\n",
      "most\n",
      "other\n",
      "some\n",
      "such\n",
      "no\n",
      "nor\n",
      "not\n",
      "only\n",
      "own\n",
      "same\n",
      "so\n",
      "than\n",
      "too\n",
      "very\n",
      "s\n",
      "t\n",
      "can\n",
      "will\n",
      "just\n",
      "don\n",
      "don't\n",
      "should\n",
      "should've\n",
      "now\n",
      "d\n",
      "ll\n",
      "m\n",
      "o\n",
      "re\n",
      "ve\n",
      "y\n",
      "ain\n",
      "aren\n",
      "aren't\n",
      "couldn\n",
      "couldn't\n",
      "didn\n",
      "didn't\n",
      "doesn\n",
      "doesn't\n",
      "hadn\n",
      "hadn't\n",
      "hasn\n",
      "hasn't\n",
      "haven\n",
      "haven't\n",
      "isn\n",
      "isn't\n",
      "ma\n",
      "mightn\n",
      "mightn't\n",
      "mustn\n",
      "mustn't\n",
      "needn\n",
      "needn't\n",
      "shan\n",
      "shan't\n",
      "shouldn\n",
      "shouldn't\n",
      "wasn\n",
      "wasn't\n",
      "weren\n",
      "weren't\n",
      "won\n",
      "won't\n",
      "wouldn\n",
      "wouldn't\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stopWords = stopwords.words('english')\n",
    "for w in stopWords:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any rules that seem to sum up this list?\n",
    "\n",
    "- pronouns\n",
    "- prepositions\n",
    "- single letters\n",
    "- conjunctions of the above categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also note that in our research, we will be removing punctuation, so we do that with these package words now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some\n",
      "should\n",
      "having\n",
      "y\n",
      "m\n",
      "himself\n",
      "through\n",
      "it\n",
      "youre\n",
      "up\n",
      "mustnt\n",
      "is\n",
      "them\n",
      "further\n",
      "werent\n",
      "isnt\n",
      "but\n",
      "him\n",
      "wouldnt\n",
      "its\n",
      "me\n",
      "hasnt\n",
      "isn\n",
      "didnt\n",
      "what\n",
      "thatll\n",
      "or\n",
      "by\n",
      "shouldve\n",
      "hadn\n",
      "here\n",
      "ours\n",
      "his\n",
      "where\n",
      "had\n",
      "which\n",
      "more\n",
      "each\n",
      "was\n",
      "have\n",
      "not\n",
      "the\n",
      "and\n",
      "once\n",
      "re\n",
      "during\n",
      "with\n",
      "yourself\n",
      "shouldnt\n",
      "being\n",
      "an\n",
      "ll\n",
      "why\n",
      "herself\n",
      "are\n",
      "were\n",
      "other\n",
      "ourselves\n",
      "after\n",
      "youll\n",
      "off\n",
      "hasn\n",
      "a\n",
      "then\n",
      "until\n",
      "couldnt\n",
      "if\n",
      "under\n",
      "our\n",
      "d\n",
      "hadnt\n",
      "we\n",
      "about\n",
      "o\n",
      "any\n",
      "won\n",
      "shouldn\n",
      "yours\n",
      "t\n",
      "haven\n",
      "myself\n",
      "am\n",
      "they\n",
      "when\n",
      "such\n",
      "themselves\n",
      "for\n",
      "don\n",
      "only\n",
      "your\n",
      "didn\n",
      "can\n",
      "as\n",
      "ma\n",
      "i\n",
      "has\n",
      "needn\n",
      "weren\n",
      "all\n",
      "wasnt\n",
      "of\n",
      "shes\n",
      "her\n",
      "there\n",
      "neednt\n",
      "into\n",
      "did\n",
      "mightn\n",
      "this\n",
      "than\n",
      "very\n",
      "wouldn\n",
      "mustn\n",
      "yourselves\n",
      "out\n",
      "aren\n",
      "doesn\n",
      "because\n",
      "nor\n",
      "that\n",
      "arent\n",
      "s\n",
      "against\n",
      "just\n",
      "do\n",
      "she\n",
      "ve\n",
      "these\n",
      "my\n",
      "to\n",
      "now\n",
      "ain\n",
      "doesnt\n",
      "over\n",
      "youd\n",
      "their\n",
      "does\n",
      "at\n",
      "wasn\n",
      "how\n",
      "before\n",
      "shant\n",
      "havent\n",
      "between\n",
      "who\n",
      "so\n",
      "itself\n",
      "hers\n",
      "couldn\n",
      "down\n",
      "he\n",
      "shan\n",
      "youve\n",
      "above\n",
      "below\n",
      "been\n",
      "in\n",
      "wont\n",
      "same\n",
      "no\n",
      "on\n",
      "dont\n",
      "be\n",
      "those\n",
      "own\n",
      "again\n",
      "theirs\n",
      "most\n",
      "while\n",
      "doing\n",
      "too\n",
      "from\n",
      "you\n",
      "mightnt\n",
      "few\n",
      "will\n",
      "both\n",
      "whom\n"
     ]
    }
   ],
   "source": [
    "stopWords = set([word.replace(\"'\", \"\") for word in stopWords])\n",
    "for word in stopWords:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the package words don't include every single individual letter, we add those in now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleLetters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "for letter in singleLetters:\n",
    "    stopWords.add(letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the most frequent words in our doctor/patient conversations and discern a few categories from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in dataframe\n",
    "transcript_df = pd.read_csv(\"/Users/samanthagarland/Downloads/processed_transcripts1.csv\")\n",
    "\n",
    "#select conversation 1\n",
    "t1 = transcript_df[\"Convo_1\"]\n",
    "\n",
    "#for simple frequency analysis, we assemble all conversations into one long string\n",
    "all_conversations_string = \"\"\n",
    "for s in t1:\n",
    "    if type(s) is str:\n",
    "        all_conversations_string += s\n",
    "\n",
    "#assemble a list of all words\n",
    "all_words = []\n",
    "for word in all_conversations_string.split(\" \"):\n",
    "     if word is not \"\" and word is not \" \":\n",
    "        all_words.append(word.lower())\n",
    "        \n",
    "#create a counter to find the most frequent words\n",
    "from collections import Counter\n",
    "counts = Counter(all_words)\n",
    "\n",
    "k = 500\n",
    "\n",
    "most_common = counts.most_common(k)\n",
    "topWords = []\n",
    "for word, num in most_common:\n",
    "    if word not in stopWords:\n",
    "        topWords.append((word, num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the kth most frequent words? Are they useful? Let's see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt 52917\n",
      "md 49270\n",
      "okay 22986\n",
      "know 20976\n",
      "um 20477\n",
      "yeah 16969\n",
      "prostate 12863\n",
      "cancer 11883\n",
      "right 11487\n",
      "surgery 10300\n",
      "thats 9885\n",
      "like 9541\n",
      "radiation 9335\n",
      "would 9301\n",
      "well 8361\n",
      "get 7807\n",
      "hmm 7691\n",
      "uh 7172\n",
      "think 7045\n",
      "one 6550\n",
      "im 6087\n",
      "go 5644\n",
      "good 5365\n",
      "theres 5034\n",
      "kind 4907\n",
      "going 4877\n",
      "mm 4845\n",
      "see 4726\n",
      "risk 4367\n",
      "back 4361\n",
      "want 4192\n",
      "oth 4157\n",
      "treatment 4131\n",
      "little 4089\n",
      "time 3991\n",
      "biopsy 3948\n",
      "mean 3887\n",
      "say 3826\n",
      "people 3734\n",
      "oh 3720\n",
      "take 3574\n",
      "years 3559\n",
      "things 3501\n",
      "really 3423\n",
      "probably 3357\n",
      "something 3309\n",
      "thing 2979\n",
      "psa 2941\n",
      "side 2920\n",
      "make 2915\n",
      "sure 2856\n",
      "talk 2715\n",
      "two 2656\n",
      "low 2591\n",
      "could 2567\n",
      "alright 2567\n",
      "got 2559\n",
      "lot 2517\n",
      "come 2401\n",
      "bit 2387\n",
      "way 2373\n",
      "gleason 2370\n",
      "said 2360\n",
      "need 2331\n",
      "months 2270\n",
      "men 2265\n",
      "done 2205\n",
      "give 2173\n",
      "anything 2148\n",
      "weeks 2144\n",
      "much 2075\n",
      "look 2035\n",
      "gonna 2030\n",
      "year 1998\n",
      "long 1992\n",
      "ill 1959\n",
      "yes 1942\n",
      "still 1940\n",
      "day 1925\n",
      "may 1899\n",
      "ah 1866\n",
      "tell 1845\n",
      "effects 1841\n",
      "bladder 1814\n",
      "actually 1804\n",
      "pretty 1792\n",
      "blood 1783\n",
      "dr 1776\n",
      "three 1755\n",
      "six 1752\n",
      "put 1744\n",
      "disease 1712\n",
      "even 1695\n",
      "surveillance 1684\n",
      "first 1641\n",
      "theyre 1638\n",
      "better 1623\n",
      "feel 1614\n",
      "usually 1608\n",
      "call 1592\n",
      "ive 1591\n",
      "different 1571\n",
      "high 1570\n",
      "options 1529\n",
      "let 1522\n",
      "every 1497\n",
      "active 1482\n",
      "us 1482\n",
      "also 1454\n",
      "maybe 1434\n",
      "might 1430\n",
      "erections 1414\n",
      "percent 1410\n",
      "function 1391\n",
      "problems 1386\n",
      "option 1378\n",
      "urinary 1361\n",
      "aggressive 1352\n",
      "patients 1340\n",
      "therapy 1339\n",
      "another 1331\n",
      "urine 1324\n",
      "chance 1319\n",
      "work 1291\n",
      "less 1275\n",
      "cant 1267\n",
      "couple 1264\n",
      "point 1253\n",
      "cause 1248\n",
      "small 1231\n",
      "next 1217\n",
      "sometimes 1209\n",
      "treat 1199\n",
      "questions 1182\n",
      "five 1176\n",
      "either 1170\n",
      "ok 1170\n",
      "around 1153\n",
      "score 1132\n",
      "problem 1119\n",
      "yep 1086\n",
      "sort 1077\n",
      "big 1072\n",
      "days 1070\n",
      "getting 1052\n",
      "week 1050\n",
      "able 1048\n",
      "terms 1044\n",
      "basically 1044\n",
      "everything 1040\n",
      "erectile 1035\n",
      "id 1027\n",
      "lets 1011\n",
      "catheter 1006\n",
      "try 988\n",
      "type 987\n",
      "guys 983\n",
      "far 973\n",
      "number 972\n",
      "fine 972\n",
      "never 971\n",
      "whats 969\n",
      "decision 969\n",
      "use 961\n",
      "check 957\n",
      "ten 944\n",
      "help 943\n",
      "hes 935\n",
      "intermediate 929\n",
      "stuff 922\n",
      "treatments 913\n",
      "always 910\n",
      "last 894\n",
      "today 878\n",
      "home 877\n",
      "heart 873\n",
      "based 866\n",
      "nerves 865\n",
      "typically 864\n",
      "huh 863\n",
      "exactly 855\n",
      "best 850\n",
      "nothing 850\n",
      "comes 847\n",
      "life 843\n",
      "incontinence 838\n",
      "higher 837\n",
      "called 837\n",
      "leakage 837\n",
      "four 836\n",
      "great 819\n",
      "else 817\n",
      "grade 812\n",
      "biopsies 808\n",
      "control 804\n",
      "cancers 801\n",
      "start 801\n",
      "part 794\n",
      "normal 791\n",
      "many 785\n",
      "question 783\n",
      "area 780\n",
      "goes 779\n",
      "talking 778\n",
      "information 769\n",
      "means 767\n",
      "thank 763\n",
      "doctor 759\n",
      "bad 757\n",
      "symptoms 755\n",
      "age 754\n",
      "seeds 751\n",
      "understand 745\n",
      "whether 742\n",
      "case 741\n",
      "wait 735\n",
      "tissue 734\n",
      "went 734\n",
      "whole 732\n",
      "away 731\n",
      "seven 730\n",
      "recommend 728\n",
      "exam 722\n",
      "procedure 715\n",
      "ago 710\n",
      "least 709\n",
      "whatever 709\n",
      "term 704\n",
      "end 703\n",
      "risks 701\n",
      "ya 700\n",
      "keep 697\n",
      "saying 694\n",
      "nice 690\n",
      "ever 688\n",
      "mmhmm 688\n",
      "cores 686\n",
      "pain 684\n",
      "guess 676\n",
      "patient 675\n",
      "taking 672\n",
      "old 669\n",
      "rectum 667\n",
      "looks 665\n",
      "live 664\n",
      "cells 662\n",
      "though 661\n",
      "took 659\n",
      "open 657\n",
      "meet 653\n",
      "watch 650\n",
      "robotic 650\n",
      "looking 646\n",
      "already 645\n",
      "hospital 645\n",
      "ask 641\n",
      "physician 636\n",
      "second 627\n",
      "find 626\n",
      "since 624\n",
      "decide 623\n",
      "half 620\n",
      "reason 619\n",
      "sexual 613\n",
      "likely 613\n",
      "spread 611\n",
      "rectal 607\n",
      "night 598\n",
      "dysfunction 597\n",
      "lymph 596\n",
      "remove 593\n",
      "urethra 589\n",
      "someone 585\n",
      "positive 583\n",
      "month 582\n",
      "times 582\n",
      "stay 582\n",
      "care 581\n",
      "correct 580\n",
      "certainly 578\n",
      "young 576\n",
      "appointment 574\n",
      "external 571\n",
      "thought 571\n",
      "meaning 567\n",
      "treated 567\n",
      "outside 565\n",
      "set 563\n",
      "left 551\n",
      "belly 550\n",
      "gets 547\n",
      "read 547\n",
      "enough 546\n",
      "family 544\n",
      "hard 544\n",
      "gland 543\n",
      "worse 543\n",
      "volume 543\n",
      "often 540\n",
      "happen 539\n",
      "healthy 539\n",
      "front 539\n",
      "told 538\n",
      "bleeding 535\n",
      "takes 534\n",
      "later 534\n",
      "theyll 532\n",
      "issues 532\n",
      "body 528\n",
      "wed 527\n",
      "within 527\n",
      "almost 526\n",
      "real 525\n",
      "guy 523\n",
      "mmm 522\n",
      "talked 521\n",
      "nerve 519\n",
      "beam 517\n",
      "growing 513\n",
      "coming 508\n",
      "change 507\n",
      "nodes 503\n",
      "idea 499\n",
      "found 497\n",
      "results 496\n",
      "wanted 495\n",
      "recovery 493\n",
      "afterwards 488\n",
      "course 488\n",
      "medical 488\n",
      "general 487\n",
      "history 485\n",
      "sorry 483\n",
      "fact 482\n",
      "issue 482\n",
      "follow 480\n",
      "weve 479\n",
      "came 476\n",
      "important 473\n",
      "bowel 465\n",
      "sounds 465\n",
      "mind 462\n",
      "cure 461\n",
      "happens 459\n",
      "makes 456\n",
      "test 456\n",
      "slow 453\n",
      "obviously 453\n",
      "die 449\n",
      "reasonable 448\n",
      "pressure 446\n",
      "va 442\n",
      "eight 440\n",
      "hear 440\n",
      "trying 439\n",
      "erection 439\n",
      "stage 438\n",
      "leave 438\n",
      "quite 437\n",
      "absolutely 435\n",
      "difference 434\n",
      "used 429\n",
      "affect 429\n",
      "amount 428\n",
      "thinking 426\n",
      "sense 426\n",
      "significant 425\n",
      "road 416\n",
      "surgeries 416\n",
      "hum 416\n",
      "legend 415\n",
      "doctors 415\n",
      "younger 414\n",
      "somebody 413\n",
      "dose 411\n",
      "surgeon 409\n",
      "likelihood 408\n",
      "cut 408\n",
      "kinda 402\n",
      "core 402\n",
      "ahead 402\n",
      "anyway 402\n",
      "longer 402\n",
      "leak 401\n",
      "together 401\n"
     ]
    }
   ],
   "source": [
    "for word, num in topWords:\n",
    "    print(word, num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these words, we see many of them are from the transcript legend. Let's create a list of those and add them to stopWords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = set([\"interview_length\", \"significant_other\", \"pt\", \"doc\", \"md\", \"oth\", \"so\", \"legend\", \"inaudible\", \"phi\", \"laughs\", \"pt/so\", \"mdmd\", \"patient\", \"physician\", \"clean\", \"indecipherable\"])\n",
    "stopWords = stopWords.union(legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list also includes many filler words, which we also add to stopWords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filler = set([\"um\", \"uhmhmm\", 'mmhmmm', \"umhmmm\", \"mmmhmm\", \"lot\", \"mmkay\", \"yer\", 'ummmm','mmmmm', \"mhmm\", \"na\", \"mkay\", \"ohhhohohohoh\", \"whatever\", \"sorta\", \"uhum\", \"noooo\", \"jeez\", \"things\", \"thing\", \"ahhhh\", \"mmmm\",\"ummm\", \"stuff\", \"yall\", \"hmm\", \"uh\", \"mm\", \"oooh\", \"uuh\",\"uhhuh\", \"uhmmm\", \"nah\", \"whatnot\", \"mhmmm\", \"uhhmm\", \"othumhmm\", \"mmhmm\", \"umhmm\",\"oh\", \"ah\", \"hm\",\"ok\", \"okay\", \"kay\", \"umm\",\"gee\", \"yeah\",\"yep\", \"huh\", \"ya\", \"mmhmm\", \"mmm\", \"hum\", \"kinda\", \"like\",\"right\", \"yup\", \"hi\", \"nope\", \"hey\", \"cuz\", \"mmhm\", \"mhm\", \"ahh\", \"hello\", \"gosh\", \"bye\", \"uhh\", \"er\", \"yea\", \"geez\", \"ohh\", \"heh\", \"ahhh\", \"ohhh\", \"aah\", \"yada\", \"whoa\", \"aaah\", \"okey\", \"dokey\", \"uhmhmmm\", \"whew\", \"unhunh\",\"nooh\", \"nahuh\", \"ahhm\", \"yeyeah\", \"uhhhhh\", \"uhhm\", \"uhm\", \"hmmm\", \"eh\", \"ha\", \"yah\", \"mmmhmmm\", \"alrighty\", \"alright\"])\n",
    "stopWords = stopWords.union(filler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list had more preposition and pronoun forms, which we include below and add to stopWords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = set([\"would\", \"well\", \"anyone\", \"shell\", \"hell\", \"etc\", \"say\", \"going\", \"look\", \"maam\", \"got\",\"let\", \"also\", \"gotta\", \"thereof\", \"lot\", \"mustve\", \"said\", \"wheres\", \"see\", \"ow\", \"went\", \"get\", \"hows\", \"hed\", \"thereve\", \"st\", \"thatd\", \"goin\", \"theyve\", \"im\", \"itd\", \"theyll\",\"go\", \"theres\", \"em\",\"thats\",\"could\", \"aint\",\"gonna\", \"ill\", \"theyre\", \"ive\", \"us\", \"cant\", \"id\", \"lets\", \"hes\", \"wed\", \"weve\", \"came\", \"sounds\", \"whats\", \"hes\", \"thered\",\"whatd\", \"doin\", \"mightve\", \"oughta\", \"gal\", \"whos\", \"itll\", \"'em\", \"wanna\", \"could\"])\n",
    "stopWords = stopWords.union(pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be absolutely no names in these transcripts, but should we find any, we will also remove them with the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = set([\"taiwan\", \"taiwanese\", \"communist\", \"henry\", \"walsh\", \"potter\", \"ohio\", \"hodgkins\", \"california\", \"florida\", \"alabama\", \"alaska\", \"germany\", \"europe\", \"michigan\", \"virginia\", \"swedish\", \"costa_rica\", \"greek\", \"african\", \"washington\", \"vietnam\", \"indianapolis\"])\n",
    "stopWords = stopWords.union(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also take out any misspelled words as we come across them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspell = set([\"thew\", \"ne\", \"de\", \"leastno\", \"un\", \"ro\", \"imrt\", \"et\", \"tthe\", \"ti\", \"youyou\", \"rd\", \"aa\", \"yepvery\", \"ga\", \"nd\", \"ab\", \"nn\", \"gu\", \"alot\", \"ay\", \"le\",\"anand\", \"andand\", \"thethe\", \"ifif\", \"itit\", \"ii\", \"iii\", \"thatsthat\", \"eek\", \"uhoh\", \"wewe\", \"isis\", \"nahuh\", \"yeyeah\",\"th\", \"'\", \"'cause\", \"youl\", \"whatev\"])\n",
    "stopWords = stopWords.union(misspell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our stop words now include words from the following categories:\n",
    "- pronouns\n",
    "- prepositions\n",
    "- single letters\n",
    "- conjunctions of the above categories\n",
    "- filler words (\"um\", \"hmm\", \"ok\", \"yep\", etc)\n",
    "- legend-specific words (\"pt\", \"md\", etc)\n",
    "- names\n",
    "- misspelled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\n",
      "'cause\n",
      "'em\n",
      "a\n",
      "aa\n",
      "aaah\n",
      "aah\n",
      "ab\n",
      "about\n",
      "above\n",
      "african\n",
      "after\n",
      "again\n",
      "against\n",
      "ah\n",
      "ahh\n",
      "ahhh\n",
      "ahhhh\n",
      "ahhm\n",
      "ain\n",
      "aint\n",
      "alabama\n",
      "alaska\n",
      "all\n",
      "alot\n",
      "alright\n",
      "alrighty\n",
      "also\n",
      "am\n",
      "an\n",
      "anand\n",
      "and\n",
      "andand\n",
      "any\n",
      "anyone\n",
      "are\n",
      "aren\n",
      "arent\n",
      "as\n",
      "at\n",
      "ay\n",
      "b\n",
      "be\n",
      "because\n",
      "been\n",
      "before\n",
      "being\n",
      "below\n",
      "between\n",
      "both\n",
      "but\n",
      "by\n",
      "bye\n",
      "c\n",
      "california\n",
      "came\n",
      "can\n",
      "cant\n",
      "clean\n",
      "communist\n",
      "costa_rica\n",
      "could\n",
      "couldn\n",
      "couldnt\n",
      "cuz\n",
      "d\n",
      "de\n",
      "did\n",
      "didn\n",
      "didnt\n",
      "do\n",
      "doc\n",
      "does\n",
      "doesn\n",
      "doesnt\n",
      "doin\n",
      "doing\n",
      "dokey\n",
      "don\n",
      "dont\n",
      "down\n",
      "during\n",
      "e\n",
      "each\n",
      "eek\n",
      "eh\n",
      "em\n",
      "er\n",
      "et\n",
      "etc\n",
      "europe\n",
      "f\n",
      "few\n",
      "florida\n",
      "for\n",
      "from\n",
      "further\n",
      "g\n",
      "ga\n",
      "gal\n",
      "gee\n",
      "geez\n",
      "germany\n",
      "get\n",
      "go\n",
      "goin\n",
      "going\n",
      "gonna\n",
      "gosh\n",
      "got\n",
      "gotta\n",
      "greek\n",
      "gu\n",
      "h\n",
      "ha\n",
      "had\n",
      "hadn\n",
      "hadnt\n",
      "has\n",
      "hasn\n",
      "hasnt\n",
      "have\n",
      "haven\n",
      "havent\n",
      "having\n",
      "he\n",
      "hed\n",
      "heh\n",
      "hell\n",
      "hello\n",
      "henry\n",
      "her\n",
      "here\n",
      "hers\n",
      "herself\n",
      "hes\n",
      "hey\n",
      "hi\n",
      "him\n",
      "himself\n",
      "his\n",
      "hm\n",
      "hmm\n",
      "hmmm\n",
      "hodgkins\n",
      "how\n",
      "hows\n",
      "huh\n",
      "hum\n",
      "i\n",
      "id\n",
      "if\n",
      "ifif\n",
      "ii\n",
      "iii\n",
      "ill\n",
      "im\n",
      "imrt\n",
      "in\n",
      "inaudible\n",
      "indecipherable\n",
      "indianapolis\n",
      "interview_length\n",
      "into\n",
      "is\n",
      "isis\n",
      "isn\n",
      "isnt\n",
      "it\n",
      "itd\n",
      "itit\n",
      "itll\n",
      "its\n",
      "itself\n",
      "ive\n",
      "j\n",
      "jeez\n",
      "just\n",
      "k\n",
      "kay\n",
      "kinda\n",
      "l\n",
      "laughs\n",
      "le\n",
      "leastno\n",
      "legend\n",
      "let\n",
      "lets\n",
      "like\n",
      "ll\n",
      "look\n",
      "lot\n",
      "m\n",
      "ma\n",
      "maam\n",
      "md\n",
      "mdmd\n",
      "me\n",
      "mhm\n",
      "mhmm\n",
      "mhmmm\n",
      "michigan\n",
      "mightn\n",
      "mightnt\n",
      "mightve\n",
      "mkay\n",
      "mm\n",
      "mmhm\n",
      "mmhmm\n",
      "mmhmmm\n",
      "mmkay\n",
      "mmm\n",
      "mmmhmm\n",
      "mmmhmmm\n",
      "mmmm\n",
      "mmmmm\n",
      "more\n",
      "most\n",
      "mustn\n",
      "mustnt\n",
      "mustve\n",
      "my\n",
      "myself\n",
      "n\n",
      "na\n",
      "nah\n",
      "nahuh\n",
      "nd\n",
      "ne\n",
      "needn\n",
      "neednt\n",
      "nn\n",
      "no\n",
      "nooh\n",
      "noooo\n",
      "nope\n",
      "nor\n",
      "not\n",
      "now\n",
      "o\n",
      "of\n",
      "off\n",
      "oh\n",
      "ohh\n",
      "ohhh\n",
      "ohhhohohohoh\n",
      "ohio\n",
      "ok\n",
      "okay\n",
      "okey\n",
      "on\n",
      "once\n",
      "only\n",
      "oooh\n",
      "or\n",
      "oth\n",
      "other\n",
      "othumhmm\n",
      "oughta\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "out\n",
      "over\n",
      "ow\n",
      "own\n",
      "p\n",
      "patient\n",
      "phi\n",
      "physician\n",
      "potter\n",
      "pt\n",
      "pt/so\n",
      "q\n",
      "r\n",
      "rd\n",
      "re\n",
      "right\n",
      "ro\n",
      "s\n",
      "said\n",
      "same\n",
      "say\n",
      "see\n",
      "shan\n",
      "shant\n",
      "she\n",
      "shell\n",
      "shes\n",
      "should\n",
      "shouldn\n",
      "shouldnt\n",
      "shouldve\n",
      "significant_other\n",
      "so\n",
      "some\n",
      "sorta\n",
      "sounds\n",
      "st\n",
      "stuff\n",
      "such\n",
      "swedish\n",
      "t\n",
      "taiwan\n",
      "taiwanese\n",
      "th\n",
      "than\n",
      "that\n",
      "thatd\n",
      "thatll\n",
      "thats\n",
      "thatsthat\n",
      "the\n",
      "their\n",
      "theirs\n",
      "them\n",
      "themselves\n",
      "then\n",
      "there\n",
      "thered\n",
      "thereof\n",
      "theres\n",
      "thereve\n",
      "these\n",
      "thethe\n",
      "thew\n",
      "they\n",
      "theyll\n",
      "theyre\n",
      "theyve\n",
      "thing\n",
      "things\n",
      "this\n",
      "those\n",
      "through\n",
      "ti\n",
      "to\n",
      "too\n",
      "tthe\n",
      "u\n",
      "uh\n",
      "uhh\n",
      "uhhhhh\n",
      "uhhm\n",
      "uhhmm\n",
      "uhhuh\n",
      "uhm\n",
      "uhmhmm\n",
      "uhmhmmm\n",
      "uhmmm\n",
      "uhoh\n",
      "uhum\n",
      "um\n",
      "umhmm\n",
      "umhmmm\n",
      "umm\n",
      "ummm\n",
      "ummmm\n",
      "un\n",
      "under\n",
      "unhunh\n",
      "until\n",
      "up\n",
      "us\n",
      "uuh\n",
      "v\n",
      "ve\n",
      "very\n",
      "vietnam\n",
      "virginia\n",
      "w\n",
      "walsh\n",
      "wanna\n",
      "was\n",
      "washington\n",
      "wasn\n",
      "wasnt\n",
      "we\n",
      "wed\n",
      "well\n",
      "went\n",
      "were\n",
      "weren\n",
      "werent\n",
      "weve\n",
      "wewe\n",
      "what\n",
      "whatd\n",
      "whatev\n",
      "whatever\n",
      "whatnot\n",
      "whats\n",
      "when\n",
      "where\n",
      "wheres\n",
      "whew\n",
      "which\n",
      "while\n",
      "who\n",
      "whoa\n",
      "whom\n",
      "whos\n",
      "why\n",
      "will\n",
      "with\n",
      "won\n",
      "wont\n",
      "would\n",
      "wouldn\n",
      "wouldnt\n",
      "x\n",
      "y\n",
      "ya\n",
      "yada\n",
      "yah\n",
      "yall\n",
      "yea\n",
      "yeah\n",
      "yep\n",
      "yepvery\n",
      "yer\n",
      "yeyeah\n",
      "you\n",
      "youd\n",
      "youl\n",
      "youll\n",
      "your\n",
      "youre\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "youve\n",
      "youyou\n",
      "yup\n",
      "z\n",
      "[\"'\", \"'cause\", \"'em\", 'a', 'aa', 'aaah', 'aah', 'ab', 'about', 'above', 'african', 'after', 'again', 'against', 'ah', 'ahh', 'ahhh', 'ahhhh', 'ahhm', 'ain', 'aint', 'alabama', 'alaska', 'all', 'alot', 'alright', 'alrighty', 'also', 'am', 'an', 'anand', 'and', 'andand', 'any', 'anyone', 'are', 'aren', 'arent', 'as', 'at', 'ay', 'b', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'bye', 'c', 'california', 'came', 'can', 'cant', 'clean', 'communist', 'costa_rica', 'could', 'couldn', 'couldnt', 'cuz', 'd', 'de', 'did', 'didn', 'didnt', 'do', 'doc', 'does', 'doesn', 'doesnt', 'doin', 'doing', 'dokey', 'don', 'dont', 'down', 'during', 'e', 'each', 'eek', 'eh', 'em', 'er', 'et', 'etc', 'europe', 'f', 'few', 'florida', 'for', 'from', 'further', 'g', 'ga', 'gal', 'gee', 'geez', 'germany', 'get', 'go', 'goin', 'going', 'gonna', 'gosh', 'got', 'gotta', 'greek', 'gu', 'h', 'ha', 'had', 'hadn', 'hadnt', 'has', 'hasn', 'hasnt', 'have', 'haven', 'havent', 'having', 'he', 'hed', 'heh', 'hell', 'hello', 'henry', 'her', 'here', 'hers', 'herself', 'hes', 'hey', 'hi', 'him', 'himself', 'his', 'hm', 'hmm', 'hmmm', 'hodgkins', 'how', 'hows', 'huh', 'hum', 'i', 'id', 'if', 'ifif', 'ii', 'iii', 'ill', 'im', 'imrt', 'in', 'inaudible', 'indecipherable', 'indianapolis', 'interview_length', 'into', 'is', 'isis', 'isn', 'isnt', 'it', 'itd', 'itit', 'itll', 'its', 'itself', 'ive', 'j', 'jeez', 'just', 'k', 'kay', 'kinda', 'l', 'laughs', 'le', 'leastno', 'legend', 'let', 'lets', 'like', 'll', 'look', 'lot', 'm', 'ma', 'maam', 'md', 'mdmd', 'me', 'mhm', 'mhmm', 'mhmmm', 'michigan', 'mightn', 'mightnt', 'mightve', 'mkay', 'mm', 'mmhm', 'mmhmm', 'mmhmmm', 'mmkay', 'mmm', 'mmmhmm', 'mmmhmmm', 'mmmm', 'mmmmm', 'more', 'most', 'mustn', 'mustnt', 'mustve', 'my', 'myself', 'n', 'na', 'nah', 'nahuh', 'nd', 'ne', 'needn', 'neednt', 'nn', 'no', 'nooh', 'noooo', 'nope', 'nor', 'not', 'now', 'o', 'of', 'off', 'oh', 'ohh', 'ohhh', 'ohhhohohohoh', 'ohio', 'ok', 'okay', 'okey', 'on', 'once', 'only', 'oooh', 'or', 'oth', 'other', 'othumhmm', 'oughta', 'our', 'ours', 'ourselves', 'out', 'over', 'ow', 'own', 'p', 'patient', 'phi', 'physician', 'potter', 'pt', 'pt/so', 'q', 'r', 'rd', 're', 'right', 'ro', 's', 'said', 'same', 'say', 'see', 'shan', 'shant', 'she', 'shell', 'shes', 'should', 'shouldn', 'shouldnt', 'shouldve', 'significant_other', 'so', 'some', 'sorta', 'sounds', 'st', 'stuff', 'such', 'swedish', 't', 'taiwan', 'taiwanese', 'th', 'than', 'that', 'thatd', 'thatll', 'thats', 'thatsthat', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'thered', 'thereof', 'theres', 'thereve', 'these', 'thethe', 'thew', 'they', 'theyll', 'theyre', 'theyve', 'thing', 'things', 'this', 'those', 'through', 'ti', 'to', 'too', 'tthe', 'u', 'uh', 'uhh', 'uhhhhh', 'uhhm', 'uhhmm', 'uhhuh', 'uhm', 'uhmhmm', 'uhmhmmm', 'uhmmm', 'uhoh', 'uhum', 'um', 'umhmm', 'umhmmm', 'umm', 'ummm', 'ummmm', 'un', 'under', 'unhunh', 'until', 'up', 'us', 'uuh', 'v', 've', 'very', 'vietnam', 'virginia', 'w', 'walsh', 'wanna', 'was', 'washington', 'wasn', 'wasnt', 'we', 'wed', 'well', 'went', 'were', 'weren', 'werent', 'weve', 'wewe', 'what', 'whatd', 'whatev', 'whatever', 'whatnot', 'whats', 'when', 'where', 'wheres', 'whew', 'which', 'while', 'who', 'whoa', 'whom', 'whos', 'why', 'will', 'with', 'won', 'wont', 'would', 'wouldn', 'wouldnt', 'x', 'y', 'ya', 'yada', 'yah', 'yall', 'yea', 'yeah', 'yep', 'yepvery', 'yer', 'yeyeah', 'you', 'youd', 'youl', 'youll', 'your', 'youre', 'yours', 'yourself', 'yourselves', 'youve', 'youyou', 'yup', 'z']\n"
     ]
    }
   ],
   "source": [
    "stopWords = sorted(list(stopWords))\n",
    "for w in stopWords:\n",
    "    print(w)\n",
    "print(stopWords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
